10 Ingenious Hacks That Helped Facebook Take Over the Internet 


Facebook’s first ten years changed the world in more ways than you think. 
Earlier this week, as Mark Zuckerberg’s social network celebrated its tenth birthday, WIRED looked back on some of its biggest innovations, including the Like, the Wall, and the Timeline. But these are merely the obvious innovations — the innovations you see every time you visit Facebook on your phone, tablet, or PC. Behind the scenes, inside the massive data centers that power this worldwide social network, you’ll find all sorts of other technologies that have changed our world in very different ways — and perhaps bigger ways.
As its social network expanded to over 1.2 billion people across the globe, Facebook engineers were forced to create new software, new machines, and even new kinds of data centers capable of efficiently serving such an enormous number of souls. It was Zuckerberg’s “Hacker Way” on an epic scale.
In one sense, the company was following in the footsteps of Google and Amazon, who had faced many of the same issues in earlier years. But unlike Google and Amazon, Facebook resolved to freely share many of its innovations with the world at large, open sourcing not only software but hardware designs in an effort to help the next generation of web companies tackle a future where online services will only continue to grow.
There were also selfish reasons for Facebook sharing all this technology. If you open source something — whether it’s software code or the blue prints for a new-age computer server — others can help you improve it. But in sharing many of the sweeping software systems it built in order to juggle data across thousands computer servers — and showing the world how it honed these servers for use inside its massive operation — Facebook quite literally changed the course of the modern data center.
Yes, it had plenty of help from other companies and independent software developers along the way. But that’s largely the point. There are certainly technologies that Facebook keeps to itself, but when it comes to the data center, the company is all about collaboration. Regardless of what you may think of the Facebook social network — or its privacy policies — Zuckerberg and company should be applauded for their swashbuckling approach to hardcore engineering.
When Zuckerburg started Facebook in 2004, he built it atop an open source database called MySQL, a standard tool for web development in those days. But as the site grew to millions of users, he and his engineers needed new ways of juggling the epic amounts of data landing on their network — the endless stream of comments, Likes, photos, and other posts. 
Luckily, both Amazon and Google had published research papers describing how they efficiently distributed their data across thousands of dirt-cheap machines. But while the two companies explained how their custom software worked, they kept the code to themselves. So two Facebook engineers — including Avinash Lakshman, who had worked on the Amazon paper — combined ideas from the two papers and built a new database called Cassandra. And in 2008, the company open sourced it, so that others could benefit too.
Today, Facebook relies more heavily on Hbase, another massive database based on Google’s BigTable paper. But Cassandra helped kickstart a movement towards the “NoSQL” database — a database that scales across myriad machines without being so religious about keeping the data in neat rows and columns — and this Facebook creation is still used by big-name companies like Netflix and Digg. It remains the second most popular NoSQL database and the third fastest growing database system in on the planet.
Hadoop, the open source tool that has redefined data analysis across the net, is another major Facebook success story. Based on two other Google research papers, Hadoop was originally developed at Yahoo, but Facebook is a big reason it has matured to the point where it’s a standard way of analyzing enormous amounts of data inside both web outfits and more traditional companies.
Outside of Yahoo, Facebook was one of the earliest companies to adopt Hadoop, and over the years, the social networking giant has worked to hone the platform in several significant ways. In its early days, for example, Hadoop was plagued by a rather large flaw that delayed its push into everyday businesses: a single number-crunching job could take down an entire cluster of servers. To fix this, the Facebook data team — lead by engineering vice president Jay Parikh — built a tool called Corona, which isolates processes so that no single job can take down the entire system.
At the same time, Parikh’s engineers expanded Hadoop to a truly global scale. The tool was originally designed for use on servers installed inside a single data center, but as Facebook grew, the team developed a way of running Hadoop across multiple computing centers in multiple geographical locations. The solution is called Prism — not to be confused with the NSA spying program. “It lets us move data around, wherever we want,” Parikh told us in 2012. “Prineville, Oregon. Forest City, North Carolina. Sweden.”
But this only begins to explain the company’s influence in the Hadoop world. Facebook is responsible for everything from Hive, a simple way of asking questions of Hadoop data, to Presto, a tool that lets you query as much as 250 petabtye data in near real-time.
Facebook was also instrumental in pushing the world towards tools that lets you more quickly retrieve information stored in the data center. Alongside Apple, the company was one of the first to replace old-fashioned hard disks with super fast flash cards from Utah outfit Fusion-io, and using a tool called Memcached, it spreads its most frequently accessed data across the memory subsystems inside thousands of servers, which provides even greater speed. Recently, the company pushed the use of memory systems even further with a tool called TAO.
Built by Facebook, TAO provides a single tool for storing data across both hard disks and memory. Some data is suited to hard disk storage, and some to memory, and TAO lets you do both. This sort of hybrid storage is the trend in the modern data center, and TAO simplifies things, giving you a common interface for all your data. Outside of Facebook, the concept is already being explored by the cloud database company Orchestrate.
In the early days of Facebook, Zuckerberg’s other big decision was to build his social network with PHP,  one of the popular programming languages of the day. PHP lets you fashion websites relatively quickly and easily, and that’s why it rose to prominence in the mid-aughts. But there’s a rub: PHP code doesn’t run as fast as code built with languages such as C++ and Java.
That’s why a crack team of Facebook engineers went to work on a tool called HHVM — short for Hip Hop Virtual Machine. This ambitious tool converts PHP code directly into machine code on the fly, as it’s executed. The story of its creation is one of the great tales in the short history of Facebook — and it could lead to a big revival in the fortunes of PHP.
But Facebook isn’t just a software innovator. It also has a knack for breaking new ground in hardware. Like Google, Facebook now designs its own servers in an effort to cut hardware and power costs across its sprawling operation. But it also goes a step further, sharing its designs with the rest of the world through its Open Compute project.
Many were skeptical when this first happened in 2011. But the idea has gone on to significantly change the worldwide hardware market, letting companies refine not only their hardware but the supply chain that moves this hardware across the globe. Many companies are now following Facebook’s lead, going straight to Asia manufacturers for inexpensive, streamlined gear, and some are even open sourcing their own hardware work, including Microsoft and cloud company Rackspace.
Through the Open Compute project, Facebook also shared designs for entire data centers. In recent years, the company has erected computing centers that use the outside air to cool their server farms — a way of saving not only money but the environment as well. It don’t need the energy-sapping chillers that typically keep servers cool.
Facebook’s data center in Prineville, Oregon operates this way. And if you want one too, you can have it. “We’ve had some people say: ‘Can we build this data center?’” Ken Patchett, the general manager of the Prineville site, told us in 2011. “And we say: ‘Of course, you can. Do you want the blueprints?’”
In some cases, Facebook open sources designs for technology it hasn’t even built yet. A prime example: the groundbreaking modular server blueprints it released last year. 
With the modular server, you can easily swap components in and out  — including parts that are typically soldered to the motherboard in today’s machines. “By modularizing the design, you can rip and place the bits that need to be upgraded, but you can leave the stuff that’s still good,” Facebook hardware guru Frank Frankovsky told us last year, pointing to memory and flash storage as hardware that you don’t have to replace as often as the processor.
You can’t buy a modular server yet, but Intel and AMD –the two largest server chip makers — have designed reference systems for manufacturers. And Facebook is well on its way to installing such machines in its own data centers.
You know how you can move your laptop display forward and back and it stays right where you left it? Facebook hardware engineers have designed a contraption that lets data center technicians do the same thing with an 800-pound tray of hard drives. The Knox storage system makes it far easier for technicians to add and remove drives — even when the tray is well above their heads. Knox uses a “friction hinge” that moves up and down when you apply a small amount of force, but will stay still once you let go of it. And it too is open source.
As Facebook’s data centers continue to grow, its engineers have sought to refine nearly every piece of hardware inside these warehouse-sized buildings, and that includes the networking gear. The isn’t happy with the expensive and rather difficult-to-manage gear it gets from big-names like Cisco and Juniper, so it’s moving to “bare-metal” networking switches that can run any software Facebook wants.
We take that for granted with PCs and laptops and servers, but with networking gear, it’s a new thing. And it’s yet another major shift in the worldwide hardware market. Facebook has yet to make the move to bare-metal, but again, it is already sharing its plans with the outside world — and working with engineers from outside the company’s walls to make it happen.
One of the most surprising things you’ll find tucked into a Facebook data center: a wall of Mac Minis. In order to build and test its incredibly complex Facebook iPhone apps, the company runs server farms built from hundreds of Mac Minis, the tiny Apple machines that were supposed to sit on the desk in your home office.
Typically, developers test iPhone and iPad apps on a single machine. But with its Mini farms — which engineers can access over a network — Facebook can test its apps on a much larger scale. This too is a trend in the software industry. Outfits like Travis CI and Sauce Labs have their own Mac server farms, offering online services that let others test iPhone and iPad apps on a massive scale. 
But all this is just a start. Even if Facebook never signs up another user, more and more data will flood its network, and it will need still newer ways of juggling all those Likes, photos, and videos. Just last week, the company unveiled an ambitious plan to use robots and Blu-Ray discs to manage little-used content — like the old photos you rarely look at. Yes, robots and Blu-rays. It’s completely unexpected thing. And don’t be surprised if the rest of the world follows suit.
